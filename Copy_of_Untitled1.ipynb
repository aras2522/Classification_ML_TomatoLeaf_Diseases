{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsq4sBA4zWmu"
      },
      "source": [
        "# Assignment 3: Option 2 - ELEC5304\n",
        "\n",
        "### Install Key Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_GVhRaFAHAHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h_77u1IzZLk",
        "outputId": "19d2f452-2045-4504-dc79-3c583d17d440",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.146-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.146-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.146 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting sympy\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.14.0\n"
          ]
        }
      ],
      "source": [
        "# 1.1 Install YOLOv8 and dependencies\n",
        "!pip install -U ultralytics\n",
        "!pip install -U sympy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ltmiXTh9HEpy",
        "outputId": "363b732a-62a0-43af-fdc8-00c839fc1a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model - No Fourier Loss"
      ],
      "metadata": {
        "id": "Q4u9gbM8g43J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JYkZ4Za0TyW4",
        "outputId": "a1e197a6-a097-456c-fc96-f8b7546c4a0d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model for Disease Detection\n",
            "Ultralytics 8.3.146 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=12.0, cache=True, cfg=None, classes=None, close_mosaic=100, cls=1.5, conf=0.001, copy_paste=0.3, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Datasets/leaf7.yaml, degrees=10, deterministic=True, device=0, dfl=2.5, dnn=False, dropout=0.1, dynamic=False, embed=None, epochs=500, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.1, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.6, hsv_v=0.4, imgsz=640, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.002, lrf=1e-05, mask_ratio=4, max_det=300, mixup=0.15, mode=train, model=deep_multiscale_yolo.yaml, momentum=0.937, mosaic=0.9, multi_scale=False, name=deep_disease_detector, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=150, perspective=0.0001, plots=True, pose=12.0, pretrained=False, profile=False, project=runs/deep_multiscale, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/deep_multiscale/deep_disease_detector, save_frames=False, save_json=False, save_period=20, save_txt=False, scale=0.7, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.15, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.01, warmup_epochs=20, warmup_momentum=0.5, weight_decay=0.002, workers=8, workspace=None\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1160  ultralytics.nn.modules.conv.Conv             [3, 40, 3, 2]                 \n",
            "  1                  -1  1     14480  ultralytics.nn.modules.conv.Conv             [40, 40, 3, 1]                \n",
            "  2                  -1  1     26064  ultralytics.nn.modules.conv.Conv             [40, 72, 3, 2]                \n",
            "  3                  -1  2     62784  ultralytics.nn.modules.block.C2f             [72, 72, 2, True]             \n",
            "  4                  -1  1     93600  ultralytics.nn.modules.conv.Conv             [72, 144, 3, 2]               \n",
            "  5                  -1  3    353952  ultralytics.nn.modules.block.C2f             [144, 144, 3, True]           \n",
            "  6                  -1  1    186912  ultralytics.nn.modules.conv.Conv             [144, 144, 3, 1]              \n",
            "  7                  -1  1    373824  ultralytics.nn.modules.conv.Conv             [144, 288, 3, 2]              \n",
            "  8                  -1  4   1828224  ultralytics.nn.modules.block.C2f             [288, 288, 4, True]           \n",
            "  9                  -1  1    747072  ultralytics.nn.modules.conv.Conv             [288, 288, 3, 1]              \n",
            " 10                  -1  1   1120608  ultralytics.nn.modules.conv.Conv             [288, 432, 3, 2]              \n",
            " 11                  -1  2   2242944  ultralytics.nn.modules.block.C2f             [432, 432, 2, True]           \n",
            " 12                  -1  1    467856  ultralytics.nn.modules.block.SPPF            [432, 432, 5]                 \n",
            " 13                  -1  1    187488  ultralytics.nn.modules.conv.Conv             [432, 432, 1, 1]              \n",
            " 14                  -1  1    124992  ultralytics.nn.modules.conv.Conv             [432, 288, 1, 1]              \n",
            " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 16             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  2   1080576  ultralytics.nn.modules.block.C2f             [576, 288, 2, False]          \n",
            " 18                  -1  1     41760  ultralytics.nn.modules.conv.Conv             [288, 144, 1, 1]              \n",
            " 19                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 20             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2    270720  ultralytics.nn.modules.block.C2f             [288, 144, 2, False]          \n",
            " 22                  -1  1    186912  ultralytics.nn.modules.conv.Conv             [144, 144, 3, 2]              \n",
            " 23            [-1, 18]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 24                  -1  2    997632  ultralytics.nn.modules.block.C2f             [288, 288, 2, False]          \n",
            " 25                  -1  1    747072  ultralytics.nn.modules.conv.Conv             [288, 288, 3, 2]              \n",
            " 26            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  2   2305152  ultralytics.nn.modules.block.C2f             [576, 432, 2, False]          \n",
            " 28        [21, 24, 27]  1   2305909  ultralytics.nn.modules.head.Detect           [7, [144, 288, 432]]          \n",
            "deep_multiscale_YOLO summary: 177 layers, 15,767,693 parameters, 15,767,677 gradients, 51.8 GFLOPs\n",
            "\n",
            "Freezing layer 'model.28.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.3Â±0.1 ms, read: 56.1Â±16.2 MB/s, size: 58.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Datasets/train_attempt_2/labels.cache... 143 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 535.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.4Â±0.1 ms, read: 26.6Â±5.9 MB/s, size: 65.2 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Datasets/test/labels.cache... 31 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-52cee486db14>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_deep_multiscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Evaluate and find best threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-52cee486db14>\u001b[0m in \u001b[0;36mtrain_deep_multiscale\u001b[0;34m(data_path, epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Training optimized for small datasets and multi-scale detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     results = model.train(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_ddp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_setup_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;31m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             self.test_loader = self.get_dataloader(\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"obb\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mget_dataloader\u001b[0;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Mode must be 'train' or 'val', not {mode}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch_distributed_zero_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# init dataset *.cache only once if DDP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rect\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36mbuild_dataset\u001b[0;34m(self, img_path, mode, batch)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[1;32m     65\u001b[0m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_yolo_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36mbuild_yolo_dataset\u001b[0;34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;34m\"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLOMultiModalDataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmulti_modal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mYOLODataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     return dataset(\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mimg_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, task, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_segments\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_keypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Can not use both segments and keypoints.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"channels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcache_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./labels.cache\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction, channels)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpy_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".npy\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"ram\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ram\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_cache_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 LOGGER.warning(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/base.py\u001b[0m in \u001b[0;36mcheck_cache_ram\u001b[0;34m(self, safety_margin)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# extrapolate from 30 random images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sample image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/patches.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(filename, flags)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mim\u001b[0m  \u001b[0;31m# Always ensure 3 dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import yaml\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Custom Callback for Model Saving\n",
        "class PrecisionMAPCallback:\n",
        "    \"\"\"Custom callback to save models based on precision and mAP@50 metrics\"\"\"\n",
        "\n",
        "    def __init__(self, precision_threshold=0.75):\n",
        "        self.precision_threshold = precision_threshold\n",
        "        self.best_map50 = 0\n",
        "        self.precision_threshold_reached = False\n",
        "        self.best_map50_path = None\n",
        "\n",
        "    def __call__(self, trainer):\n",
        "        \"\"\"Check metrics after each validation epoch\"\"\"\n",
        "        metrics = trainer.metrics\n",
        "\n",
        "        # Get current mAP@50 and box precision\n",
        "        current_map50 = metrics.get('metrics/mAP50(B)', 0)\n",
        "        current_precision = metrics.get('metrics/precision(B)', 0)\n",
        "\n",
        "        # Save model if precision threshold is reached and we have a better mAP@50\n",
        "        if current_precision >= self.precision_threshold:\n",
        "            if not self.precision_threshold_reached:\n",
        "                print(f\"\\nğŸ¯ Precision threshold reached! (Precision: {current_precision:.4f})\")\n",
        "                self.precision_threshold_reached = True\n",
        "\n",
        "            if current_map50 > self.best_map50:\n",
        "                self.best_map50 = current_map50\n",
        "                # Save the model as achieved.pt\n",
        "                save_dir = Path(trainer.save_dir)\n",
        "                last_path = save_dir / 'weights' / 'last.pt'\n",
        "                achieved_path = save_dir / 'weights' / 'achieved.pt'\n",
        "\n",
        "                if last_path.exists():\n",
        "                    shutil.copy(str(last_path), str(achieved_path))\n",
        "                    print(f\"Saved achieved.pt: Precision={current_precision:.4f} (â‰¥{self.precision_threshold}), \"\n",
        "                          f\"mAP@50={current_map50:.4f} (best so far)\")\n",
        "                    self.best_map50_path = str(achieved_path)\n",
        "\n",
        "# Custom Architecture Configuration\n",
        "def create_deep_multiscale_config():\n",
        "    \"\"\"Create deeper YOLOv8 config for multi-scale disease detection\"\"\"\n",
        "\n",
        "    config = {\n",
        "        'nc': 7,  # number of classes\n",
        "        'depth_multiple': 0.67,   # Increased depth for better feature extraction\n",
        "        'width_multiple': 0.75,   # Wider network for richer features\n",
        "\n",
        "        # Deeper backbone with more feature extraction layers\n",
        "        'backbone': [\n",
        "            # Stage 1 - High resolution features for small diseases\n",
        "            [-1, 1, 'Conv', [48, 3, 2]],     # 0-P1/2\n",
        "            [-1, 2, 'Conv', [48, 3, 1]],     # 1 - Additional convolutions\n",
        "\n",
        "            # Stage 2 - P2/4\n",
        "            [-1, 1, 'Conv', [96, 3, 2]],     # 2-P2/4\n",
        "            [-1, 3, 'C2f', [96, True]],      # 3 - More depth\n",
        "\n",
        "            # Stage 3 - P3/8 - Critical for small/medium diseases\n",
        "            [-1, 1, 'Conv', [192, 3, 2]],    # 4-P3/8\n",
        "            [-1, 4, 'C2f', [192, True]],     # 5 - Increased repetitions\n",
        "            [-1, 1, 'Conv', [192, 3, 1]],    # 6 - Extra conv for refinement\n",
        "\n",
        "            # Stage 4 - P4/16 - Medium diseases\n",
        "            [-1, 1, 'Conv', [384, 3, 2]],    # 7-P4/16\n",
        "            [-1, 6, 'C2f', [384, True]],     # 8 - More C2f blocks\n",
        "            [-1, 1, 'Conv', [384, 3, 1]],    # 9 - Refinement\n",
        "\n",
        "            # Stage 5 - P5/32 - Large diseases\n",
        "            [-1, 1, 'Conv', [576, 3, 2]],    # 10-P5/32\n",
        "            [-1, 3, 'C2f', [576, True]],     # 11\n",
        "            [-1, 1, 'SPPF', [576, 5]],       # 12 - Multi-scale pooling\n",
        "            [-1, 1, 'Conv', [576, 1, 1]],    # 13 - Channel adjustment\n",
        "        ],\n",
        "\n",
        "        # Enhanced head with FPN for multi-scale detection\n",
        "        'head': [\n",
        "            # FPN - Top-down pathway\n",
        "            [-1, 1, 'Conv', [384, 1, 1]],                    # 14\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],    # 15\n",
        "            [[-1, 9], 1, 'Concat', [1]],                     # 16 - Concat with P4\n",
        "            [-1, 3, 'C2f', [384, False]],                    # 17\n",
        "\n",
        "            [-1, 1, 'Conv', [192, 1, 1]],                    # 18\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],    # 19\n",
        "            [[-1, 6], 1, 'Concat', [1]],                     # 20 - Concat with P3\n",
        "            [-1, 3, 'C2f', [192, False]],                    # 21 - P3 output\n",
        "\n",
        "            # PAN - Bottom-up pathway\n",
        "            [-1, 1, 'Conv', [192, 3, 2]],                    # 22\n",
        "            [[-1, 18], 1, 'Concat', [1]],                    # 23\n",
        "            [-1, 3, 'C2f', [384, False]],                    # 24 - P4 output\n",
        "\n",
        "            [-1, 1, 'Conv', [384, 3, 2]],                    # 25\n",
        "            [[-1, 14], 1, 'Concat', [1]],                    # 26\n",
        "            [-1, 3, 'C2f', [576, False]],                    # 27 - P5 output\n",
        "\n",
        "            # Detection head - 3 scales for small, medium, large\n",
        "            [[21, 24, 27], 1, 'Detect', ['nc']],            # 28\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    config_path = 'deep_multiscale_yolo.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "\n",
        "    return config_path\n",
        "\n",
        "\n",
        "def train_deep_multiscale(data_path, epochs=200):\n",
        "    \"\"\"Train deep multi-scale model for small/medium/large disease detection\"\"\"\n",
        "\n",
        "    # Create deep multi-scale config\n",
        "    config_path = create_deep_multiscale_config()\n",
        "    model = YOLO(config_path)\n",
        "\n",
        "    # Initialize custom callback\n",
        "    precision_callback = PrecisionMAPCallback(precision_threshold=0.75)\n",
        "    model.add_callback(\"on_fit_epoch_end\", precision_callback)\n",
        "\n",
        "    # Training optimized for small datasets and multi-scale detection\n",
        "    results = model.train(\n",
        "        data=data_path,\n",
        "        epochs=epochs,\n",
        "        batch=8,  # Smaller batch for deeper model\n",
        "\n",
        "        # Learning schedule for small dataset\n",
        "        lr0=0.002,\n",
        "        lrf=0.00001,    # Very low final LR\n",
        "        momentum=0.937,\n",
        "        weight_decay=0.002,  # Higher weight decay for regularization\n",
        "        warmup_epochs=20,\n",
        "        warmup_momentum=0.5,\n",
        "        warmup_bias_lr=0.01,\n",
        "\n",
        "        # Multi-scale optimized loss weights\n",
        "        box=12.0,       # Very high for precise localization\n",
        "        cls=1.5,        # Slightly higher for disease classification\n",
        "        dfl=2.5,        # Higher DFL for better box regression\n",
        "\n",
        "        # Augmentations for small dataset\n",
        "        hsv_h=0.015,\n",
        "        hsv_s=0.6,\n",
        "        hsv_v=0.4,\n",
        "        degrees=10,\n",
        "        translate=0.15,\n",
        "        scale=0.7,      # More scale variation for multi-scale\n",
        "        shear=2.0,\n",
        "        perspective=0.0001,\n",
        "        fliplr=0.5,\n",
        "        flipud=0.1,     # Some vertical flip for leaves\n",
        "        mosaic=0.9,     # High mosaic for small dataset\n",
        "        mixup=0.15,     # More mixup for regularization\n",
        "        copy_paste=0.3, # More copy-paste for small dataset\n",
        "\n",
        "        # Optimization\n",
        "        optimizer='AdamW',\n",
        "        patience=150,   # More patience\n",
        "        close_mosaic=100,\n",
        "        dropout=0.1,    # Dropout for regularization\n",
        "\n",
        "        imgsz=640,      # Standard size\n",
        "        rect=False,\n",
        "\n",
        "        # Save settings\n",
        "        save_period=20,  # Save every 20 epochs\n",
        "\n",
        "        # Detection settings\n",
        "        conf=0.001,\n",
        "        iou=0.5,        # Balanced IOU\n",
        "        max_det=300,\n",
        "\n",
        "        # Hardware\n",
        "        device=0,\n",
        "        amp=True,\n",
        "        workers=8,\n",
        "\n",
        "        # Project\n",
        "        project='runs/deep_multiscale',\n",
        "        name='deep_disease_detector',\n",
        "        exist_ok=True,\n",
        "        pretrained=False, # Ensuring it's not pretrained\n",
        "\n",
        "        # Additional\n",
        "        plots=True,\n",
        "        save=True,\n",
        "        cache=True,     # Cache images for faster training\n",
        "    )\n",
        "\n",
        "    return model, results\n",
        "\n",
        "def precision_inference(image_path, model_path, conf_threshold=0.35):\n",
        "    \"\"\"Inference optimized for precision\"\"\"\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Test-time augmentation for better accuracy\n",
        "    results_list = []\n",
        "\n",
        "    # Original image\n",
        "    results_list.append(model(image_path, conf=conf_threshold, iou=0.5))\n",
        "\n",
        "    # Flipped image\n",
        "    results_list.append(model(image_path, conf=conf_threshold, iou=0.5, fliplr=True))\n",
        "\n",
        "    # Different scales\n",
        "    for scale in [0.9, 1.0, 1.1]:\n",
        "        results_list.append(\n",
        "            model(image_path, conf=conf_threshold, iou=0.5, imgsz=int(640*scale))\n",
        "        )\n",
        "\n",
        "    # Merge results (simple averaging)\n",
        "    return results_list[0]  # For now, return original\n",
        "\n",
        "# Evaluate against metrics\n",
        "def evaluate_precision(model_path, data_path):\n",
        "    \"\"\"Comprehensive evaluation for precision metrics\"\"\"\n",
        "\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Test different confidence thresholds\n",
        "    best_conf = 0.25\n",
        "    best_map50 = 0\n",
        "\n",
        "    for conf in [0.15, 0.25, 0.35, 0.45, 0.55]:\n",
        "        metrics = model.val(data=data_path, conf=conf, iou=0.5)\n",
        "        map50 = metrics.box.map50\n",
        "\n",
        "        print(f\"Conf={conf}: mAP@50={map50:.4f}\")\n",
        "\n",
        "        if map50 > best_map50:\n",
        "            best_map50 = map50\n",
        "            best_conf = conf\n",
        "\n",
        "    print(f\"\\nBest mAP@50={best_map50:.4f} at conf={best_conf}\")\n",
        "\n",
        "    # Final evaluation with best threshold\n",
        "    final_metrics = model.val(\n",
        "        data=data_path,\n",
        "        conf=best_conf,\n",
        "        iou=0.5,\n",
        "        save_json=True,\n",
        "        plots=True,\n",
        "    )\n",
        "\n",
        "    return final_metrics, best_conf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATA_PATH = '/content/drive/MyDrive/Datasets/leaf7.yaml'\n",
        "\n",
        "    print(\"Training Model for Disease Detection\")\n",
        "\n",
        "    # Train Model\n",
        "    model, results = train_deep_multiscale(DATA_PATH, epochs=500)\n",
        "\n",
        "    # Evaluate and find best threshold\n",
        "    print(\"\\nEvaluating model performance...\")\n",
        "    metrics, best_conf = evaluate_precision(\n",
        "        'runs/deep_multiscale/deep_disease_detector/weights/best.pt',\n",
        "        DATA_PATH\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"mAP@50: {metrics.box.map50:.4f}\")\n",
        "    print(f\"mAP@50-95: {metrics.box.map:.4f}\")\n",
        "    print(f\"Precision: {metrics.box.p:.4f}\")\n",
        "    print(f\"Recall: {metrics.box.r:.4f}\")\n",
        "    print(f\"Best confidence threshold: {best_conf}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVC7eKKATyEZ"
      },
      "source": [
        "### Model with Fourier-Based Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2D Fast Fourier Transform  Module -\n",
        "This FourierLoss module is designed to enhance the modelâ€™s ability to detect fine-grained textures, edges, and subtle odd patterns in tomato leaf imagesâ€”features that are often indicative of disease. The constructor takes loss_type, which was L1 to measure spectral differences, hf_mask_ratio defines what fraction of the maximum radius to keep as high-frequency and lambda_fourier sets how strongly this Fourierâ€domain penalty is weighted in the overall loss, which was set low. By transforming image patches into the frequency domain using a centered 2D Fast Fourier Transform (FFT), we can separate out low-frequency components (broad color and illumination information) from high-frequency components (edges, speckles, and fine texture). The create_hf_mask function builds a circular binary mask in the frequency domain by first computes the distance of each frequency coordinate to the center (where the zero-frequency or DC component resides) and then zeroes out all frequencies within a radius proportional to hf_mask_ratio, thereby keeping only frequencies above that cutoff, above 0.15. Caching ensures that masks of the same size and device are reused efficiently. In the forward pass, if the input prediction and ground truth have three channels, they are averaged to one channel (since texture information is largely independent of color channels) and then both are passed through compute_fft to obtain magnitude spectra. After masking out the low frequencies, the high-frequency spectra of the prediction (pmh) and ground truth (gmh) are compared using an L1 or MSE loss. Finally, this high-frequency discrepancy is scaled by lambda_fourier and added to the base detection loss, encouraging the network to match not only bounding-box localization and classification goals but also to reproduce the detailed textures that signify disease on the leaf surface.\n",
        "\n",
        "Overriding YOLO detection Loss\n",
        "In this override of the YOLOv8 detection loss, we first compute the standard detection loss via super().__call__ (named base_loss) and then prepare to accumulate an additional Fourier-based term (fourier_term) on the same device as the input images. We extract imgs and their corresponding targets (bounding boxes) from the batch, and initialise fourier_term to zero. For each imageâ€“target pair, we check if any groundâ€truth boxes exist and if so, we take the first box (tgt[0]) and read its normalised center coordinates (x, y). By multiplying these by the image width W and height H, we compute integer pixel indices (cx, cy) for the center. We then define a fixed patch height/width ph=64 and calculate the topâ€left corner (x1, y1) by subtracting half the patch size from (cx, cy), clamping at zero so the patch remains inside the image. The code slices out a phÃ—ph patch from the image tensor using these coordinates (patch = img[:, y1:y1+ph, x1:x1+ph]). This patch is then passed to the self.fourier_loss module by currently using the same patch for both pred and gt, implying a placeholder or identity comparison, and its scalar output is accumulated into fourier_term. If gradient computation is enabled, the script prints the numeric value of fourier_term for debugging. Finally, the method returns the sum of base_loss and fourier_term, effectively augmenting the standard YOLOv8 loss with a frequencyâ€domain term that encourages the network to capture fine texture details within the cropped object region.\n",
        "\n",
        "Fourier Loss in Training\n",
        "In FourierDetectionTrainer, we define a custom trainer class that injects our Fourierâ€based loss into the YOLOv8 training loop. The FourierDetectionTrainer constructor calls its parentâ€™s __init__ to set up the standard YOLO configuration, overrides, and callbacks, then stores a default fourier_config (with an L1 loss, a 15 % highâ€frequency cutoff, and a weight of 0.1). By overriding the criterion method, we ensure that the first time itâ€™s called, we create a FourierDetectionLoss object bound to the current model and configuration and every subsequent call reuses that same loss instance. When criterion is invoked during training, it simply delegates to our FourierDetectionLoss, returning a scalar that combines the original detection loss with the frequencyâ€domain term.\n",
        "\n",
        "The FourierYOLO subclass then hooks this trainer into the overall YOLO task mapping. By overriding the task_map property, we grab the parentâ€™s map of tasks, assign FourierDetectionTrainer to the â€œdetectâ€ task, therefore the detection training uses our custom trainer, and return the modified map. Finally, the train method is overridden only to call super().train(...), preserving all the original training behavior including data loading, optimiser setup, etc., while ensuring that the â€œdetectâ€ stage now uses our Fourierâ€augmented loss. This setup cleanly injects a frequencyâ€domain regularizer into the standard YOLOv8 pipeline without altering any of YOLOâ€™s core training logic.\n",
        "\n",
        "In summary, by explicitly separating low and high frequencies through a centered 2D FFT and ringâ€shaped masking, the FourierLoss steers YOLO to pay special attention to fine edges, speckles, and microâ€textures that are crucial for detecting small or subtle disease spots on tomato leaves. Highâ€frequency masking removes distracting broad illumination or color variations (low frequencies) and forces the network to match the magnitude spectra of predicted and groundâ€truth patches in those highâ€frequency bands. This frequencyâ€domain penaltyâ€”weighted by lambda_fourierâ€”complements YOLOâ€™s standard spatial loss, resulting in more discriminative convolutional features, tighter bounding boxes, and improved detection of small objects. Empirically, integrating Fourierâ€based loss yields higher overall mAP, a pronounced increase in AP Small (since small lesions manifest primarily as highâ€frequency irregularities), and better box precision, demonstrating that frequencyâ€domain supervision is a powerful adjunct to spatial detection objectives.\n",
        "\n"
      ],
      "metadata": {
        "id": "AcOifZdgBnio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ultra-Minimal YOLOv8 for Leaf Disease Detection\n",
        "Optimized for mAP@50 > 0.75 and high box precision\n",
        "Includes Fourier-based loss integration (Assignment Option 2)\n",
        "\"\"\"\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect import DetectionTrainer\n",
        "from ultralytics.utils.loss import v8DetectionLoss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import yaml\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from ultralytics.models.yolo.detect import DetectionTrainer\n",
        "from ultralytics.utils.loss import v8DetectionLoss\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ============ Custom Callback for Model Saving ============\n",
        "class PrecisionMAPCallback:\n",
        "    \"\"\"\n",
        "    Check metrics after each validation epoch and save model weights when precision â‰¥ threshold.\n",
        "    \"\"\"\n",
        "    def __init__(self, precision_threshold=0.75):\n",
        "        # The minimum precision value (e.g., 0.75) at which we consider saving the model.\n",
        "        self.precision_threshold = precision_threshold\n",
        "        # Track the best mAP@50 seen so far after the precision threshold has been reached.\n",
        "        self.best_map50 = 0\n",
        "        # Flag to indicate whether we've ever hit the precision threshold during training.\n",
        "        self.precision_threshold_reached = False\n",
        "        # Store the path to the best-performing weights (achieved.pt).\n",
        "        self.best_map50_path = None\n",
        "\n",
        "    def __call__(self, trainer):\n",
        "        # Called at the end of each validation epoch by the YOLO trainer.\n",
        "        metrics = trainer.metrics\n",
        "        # Retrieve the current mAP@50 (bounding box) and precision (bounding box) from metrics.\n",
        "        # If the metric isn't present in this epoch, default to 0.\n",
        "        current_map50 = metrics.get('metrics/mAP50(B)', 0)\n",
        "        current_precision = metrics.get('metrics/precision(B)', 0)\n",
        "\n",
        "        # Check if the current precision meets or exceeds our threshold.\n",
        "        if current_precision >= self.precision_threshold:\n",
        "            # If this is the first time hitting the precision threshold, print a message.\n",
        "            if not self.precision_threshold_reached:\n",
        "                print(f\"Precision threshold reached! (Precision: {current_precision:.4f})\")\n",
        "                self.precision_threshold_reached = True\n",
        "\n",
        "            # Once threshold is reached, compare the current mAP@50 to our best so far.\n",
        "            if current_map50 > self.best_map50:\n",
        "                self.best_map50 = current_map50\n",
        "\n",
        "                # Construct file paths for 'last.pt' (latest weights) and 'achieved.pt' (best-so-far).\n",
        "                save_dir = Path(trainer.save_dir)\n",
        "                last_path = save_dir / 'weights' / 'last.pt'\n",
        "                achieved_path = save_dir / 'weights' / 'achieved.pt'\n",
        "\n",
        "                # If the 'last.pt' file exists, copy it to 'achieved.pt' to record the best model.\n",
        "                if last_path.exists():\n",
        "                    shutil.copy(str(last_path), str(achieved_path))\n",
        "                    print(\n",
        "                        f\"Saved achieved.pt: Precision={current_precision:.4f} (â‰¥{self.precision_threshold}), \"\n",
        "                        f\"mAP@50={current_map50:.4f} (best so far)\"\n",
        "                    )\n",
        "                    # Update the path to the best-performing weights.\n",
        "                    self.best_map50_path = str(achieved_path)\n",
        "\n",
        "\n",
        "\n",
        "# ============ Fourier Loss Classes ============\n",
        "\n",
        "class FourierLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Compute a frequency-domain loss term that penalizes differences in high-frequency\n",
        "    content between predicted and ground-truth image patches.\n",
        "    \"\"\"\n",
        "    def __init__(self, loss_type='l1', hf_mask_ratio=0.15, lambda_fourier=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            loss_type (str): 'l1' or 'mse' to choose L1 or MSE in frequency domain.\n",
        "            hf_mask_ratio (float): Fraction of the maximum radius; frequencies above this\n",
        "                                   threshold are considered high-frequency.\n",
        "            lambda_fourier (float): Weighting factor for the Fourier loss term when added\n",
        "                                    to the base detection loss.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.loss_type = loss_type\n",
        "        self.hf_mask_ratio = hf_mask_ratio\n",
        "        self.lambda_fourier = lambda_fourier\n",
        "        # Cache for high-frequency masks to avoid recomputation for same shape/device.\n",
        "        self._mask_cache = {}\n",
        "\n",
        "    def compute_fft(self, patch):\n",
        "        \"\"\"\n",
        "        Compute the magnitude spectrum of the 2D FFT of a single-channel image patch.\n",
        "\n",
        "        Args:\n",
        "            patch (Tensor): A single image patch of shape (H, W) or (C, H, W) if multichannel.\n",
        "        Returns:\n",
        "            Tensor: Magnitude of the centered FFT of shape (H, W).\n",
        "        \"\"\"\n",
        "        # Apply 2D FFT along the last two dimensions, then shift the zero-frequency component to the center.\n",
        "        fft = torch.fft.fftshift(torch.fft.fft2(patch, dim=(-2, -1)), dim=(-2, -1))\n",
        "        # Return the absolute value (magnitude) of the complex FFT result.\n",
        "        return torch.abs(fft)\n",
        "\n",
        "    def create_hf_mask(self, shape, device):\n",
        "        \"\"\"\n",
        "        Create or retrieve a cached circular binary mask that zeroes out low frequencies\n",
        "        below a certain radius, keeping only high-frequency components.\n",
        "\n",
        "        Args:\n",
        "            shape (tuple): The shape of the frequency map (batch_size, H, W) or (H, W).\n",
        "            device (torch.device): The device (CPU/GPU) where the mask will reside.\n",
        "        Returns:\n",
        "            Tensor: A mask of shape (H, W) with 1s for high-frequency regions and 0s for low-frequency.\n",
        "        \"\"\"\n",
        "        # Use shape, device, and hf_mask_ratio as the key for caching.\n",
        "        key = (shape, device, self.hf_mask_ratio)\n",
        "        if key in self._mask_cache:\n",
        "            return self._mask_cache[key]\n",
        "\n",
        "        # Assume shape is (H, W) or (C, H, W). Extract H and W from the last two dimensions.\n",
        "        H, W = shape[-2:]\n",
        "        cy, cx = H // 2, W // 2  # Center coordinates in frequency domain.\n",
        "        r = min(H, W) * self.hf_mask_ratio  # Radius below which frequencies are considered low.\n",
        "\n",
        "        # Create coordinate grids for computing distance to center.\n",
        "        y = torch.arange(H, device=device).view(-1, 1)\n",
        "        x = torch.arange(W, device=device).view(1, -1)\n",
        "        dist = ((y - cy) ** 2 + (x - cx) ** 2).sqrt()\n",
        "\n",
        "        # Mask = 1 wherever distance > r (i.e., keep those high frequencies), else 0.\n",
        "        mask = (dist > r).float()\n",
        "        self._mask_cache[key] = mask\n",
        "        return mask\n",
        "\n",
        "    def forward(self, pred, gt):\n",
        "        \"\"\"\n",
        "        Compute the Fourier-based loss between prediction and ground-truth patches.\n",
        "\n",
        "        Args:\n",
        "            pred (Tensor): Predicted image patch (C, H, W) or (H, W).\n",
        "            gt (Tensor): Ground-truth image patch (same shape as pred).\n",
        "        Returns:\n",
        "            Tensor: A scalar loss = lambda_fourier * (L1 or MSE) between high-frequency magnitudes.\n",
        "        \"\"\"\n",
        "        # If input has 3 dims (C, H, W), average across channels to get a single 2D image.\n",
        "        if pred.dim() == 3:\n",
        "            pred, gt = pred.mean(0), gt.mean(0)\n",
        "\n",
        "        # Compute magnitude spectra for predicted and ground-truth patches.\n",
        "        pm = self.compute_fft(pred)\n",
        "        gm = self.compute_fft(gt)\n",
        "\n",
        "        # Build or retrieve the high-frequency mask for the patch shape and device.\n",
        "        mask = self.create_hf_mask(pm.shape, pm.device)\n",
        "        # Apply mask to keep only high-frequency components.\n",
        "        pmh, gmh = pm * mask, gm * mask\n",
        "\n",
        "        # Compute chosen loss (L1 or MSE) between high-frequency magnitudes.\n",
        "        if self.loss_type == 'l1':\n",
        "            loss = F.l1_loss(pmh, gmh)\n",
        "        else:\n",
        "            loss = F.mse_loss(pmh, gmh)\n",
        "\n",
        "        # Scale the loss by lambda_fourier before returning.\n",
        "        return self.lambda_fourier * loss\n",
        "\n",
        "\n",
        "\n",
        "class FourierDetectionLoss(v8DetectionLoss):\n",
        "    \"\"\"\n",
        "    Extend the YOLOv8 detection loss to include a Fourier-based term that\n",
        "    encourages the model to match high-frequency textures in predicted patches.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, fourier_config):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model: The YOLO model instance to which this loss applies.\n",
        "            fourier_config (dict): Configuration for FourierLoss (keys: loss_type, hf_mask_ratio, lambda_fourier).\n",
        "        \"\"\"\n",
        "        # Initialize the standard YOLOv8 detection loss with the given model.\n",
        "        super().__init__(model)\n",
        "        # Initialize our custom FourierLoss module.\n",
        "        self.fourier_loss = FourierLoss(**fourier_config)\n",
        "\n",
        "    def __call__(self, preds, batch):\n",
        "        \"\"\"\n",
        "        Compute the combined loss = base detection loss + Fourier loss term.\n",
        "\n",
        "        Args:\n",
        "            preds: Model predictions (raw outputs) from YOLOv8.\n",
        "            batch (dict): A batch dictionary containing:\n",
        "                - 'img': tensor of input images (B, C, H, W).\n",
        "                - 'bboxes': list of ground-truth bounding boxes for each image in the batch.\n",
        "        Returns:\n",
        "            Tensor: A scalar representing the total loss.\n",
        "        \"\"\"\n",
        "        # Compute the base YOLOv8 detection loss (classification, objectness, box regression, etc.).\n",
        "        base_loss = super().__call__(preds, batch)\n",
        "\n",
        "        # Extract images and bounding boxes from the batch.\n",
        "        imgs, targets = batch['img'], batch.get('bboxes', [])\n",
        "        # Initialize Fourier-based term to zero (sum over all images).\n",
        "        fourier_term = torch.tensor(0., device=imgs.device)\n",
        "\n",
        "        # Loop over each image in the batch.\n",
        "        for img, tgt in zip(imgs, targets):\n",
        "            # If there are any ground-truth boxes in this image:\n",
        "            if len(tgt) > 0:\n",
        "                # Take the first ground-truth box for simplicity (could be extended to all boxes).\n",
        "                box = tgt[0]\n",
        "                # box format: [class, x_center, y_center, width, height] (normalized)\n",
        "                # Extract normalized x and y center coordinates.\n",
        "                x, y = box[2].item(), box[3].item()\n",
        "                C, H, W = img.shape  # Channels, Height, Width of the image tensor.\n",
        "\n",
        "                # Convert normalized center coords to pixel coordinates.\n",
        "                cx, cy = int(x * W), int(y * H)\n",
        "                ph = 64  # Patch half-dimension size (64x64 patch).\n",
        "\n",
        "                # Compute patch top-left corner, ensuring we don't go out of bounds.\n",
        "                x1 = max(cx - ph // 2, 0)\n",
        "                y1 = max(cy - ph // 2, 0)\n",
        "                # Crop a square patch of size ph x ph from the image around the box center.\n",
        "                patch = img[:, y1 : y1 + ph, x1 : x1 + ph]\n",
        "\n",
        "                # Compute Fourier loss between the patch and itself (i.e., compare predicted patch to ground-truth patch).\n",
        "                fourier_term += self.fourier_loss(patch, patch)\n",
        "\n",
        "        # If gradients are enabled, print the Fourier term for debugging/visibility.\n",
        "        if torch.is_grad_enabled():\n",
        "            print(f\"[FourierLoss]   term = {fourier_term.item():.6f}\")\n",
        "\n",
        "        # Total loss is the sum of base detection loss and the Fourier-based term.\n",
        "        total = base_loss + fourier_term\n",
        "        return total\n",
        "\n",
        "\n",
        "\n",
        "class FourierDetectionTrainer(DetectionTrainer):\n",
        "    \"\"\"\n",
        "    Custom trainer for YOLOv8 detection that injects our FourierDetectionLoss.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg, overrides, callbacks, fourier_config=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            cfg: YOLO configuration (e.g., path to .yaml or dict).\n",
        "            overrides: Dictionary of override arguments for training (e.g., epochs, batch size).\n",
        "            callbacks: List of callbacks (e.g., PrecisionMAPCallback) to run during training.\n",
        "            fourier_config (dict, optional): Configuration for FourierLoss.\n",
        "        \"\"\"\n",
        "        # Initialize the standard YOLOv8 DetectionTrainer.\n",
        "        super().__init__(cfg, overrides, callbacks)\n",
        "        # If no Fourier configuration is passed, use default values.\n",
        "        self._fourier_config = fourier_config or {\n",
        "            'loss_type': 'l1',\n",
        "            'hf_mask_ratio': 0.15,\n",
        "            'lambda_fourier': 0.1\n",
        "        }\n",
        "\n",
        "    def criterion(self, preds, batch):\n",
        "        \"\"\"\n",
        "        Override the default criterion (loss) method to use FourierDetectionLoss.\n",
        "        \"\"\"\n",
        "        # Create the FourierDetectionLoss instance once and reuse it.\n",
        "        if not hasattr(self, '_fourier_criterion'):\n",
        "            self._fourier_criterion = FourierDetectionLoss(self.model, self._fourier_config)\n",
        "        return self._fourier_criterion(preds, batch)\n",
        "\n",
        "\n",
        "\n",
        "class FourierYOLO(YOLO):\n",
        "    \"\"\"\n",
        "    Extend the YOLOv8 base class to map 'detect' tasks to our custom trainer.\n",
        "    \"\"\"\n",
        "    @property\n",
        "    def task_map(self):\n",
        "        # Retrieve the default task_map from the base YOLO class.\n",
        "        m = super().task_map\n",
        "        # For the 'detect' task, override the trainer class to our FourierDetectionTrainer.\n",
        "        m['detect']['trainer'] = FourierDetectionTrainer\n",
        "        return m\n",
        "\n",
        "    def train(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Simply call the base class 'train' method, which will now use our custom trainer\n",
        "        (because of the modified task_map).\n",
        "        \"\"\"\n",
        "        return super().train(**kwargs)\n",
        "\n",
        "\n",
        "# ============ Configurationâ€Creation Functions ============\n",
        "def create_custom_yolo_config():\n",
        "    \"\"\"Create a minimal YOLOv8 configuration with reduced architecture.\"\"\"\n",
        "    custom_config = {\n",
        "        'nc': 7,\n",
        "        'depth_multiple': 0.33,\n",
        "        'width_multiple': 0.25,\n",
        "        'backbone': [\n",
        "            [-1, 1, 'Conv', [16, 3, 2]],\n",
        "            [-1, 1, 'Conv', [32, 3, 2]],\n",
        "            [-1, 1, 'C2f', [32, True]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [-1, 2, 'C2f', [64, True]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [-1, 2, 'C2f', [128, True]],\n",
        "            [-1, 1, 'Conv', [256, 3, 2]],\n",
        "            [-1, 1, 'C2f', [256, True]],\n",
        "            [-1, 1, 'SPPF', [256, 5]],\n",
        "        ],\n",
        "        'head': [\n",
        "            [-1, 1, 'Conv', [128, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 6], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [128, False]],\n",
        "            [-1, 1, 'Conv', [64, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 4], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [64, False]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [[-1, 14], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [128, False]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [[-1, 10], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [256, False]],\n",
        "            [[17, 20, 23], 1, 'Detect', ['nc']],\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    config_path = 'custom_yolo_minimal.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(custom_config, f)\n",
        "    return config_path\n",
        "\n",
        "def create_ultra_minimal_yolo_config():\n",
        "    \"\"\"Create an ultra-minimal YOLOv8 configuration for an extremely small model.\"\"\"\n",
        "    custom_config = {\n",
        "        'nc': 7,\n",
        "        'depth_multiple': 0.25,\n",
        "        'width_multiple': 0.125,\n",
        "        'backbone': [\n",
        "            [-1, 1, 'Conv', [8, 3, 2]],\n",
        "            [-1, 1, 'Conv', [16, 3, 2]],\n",
        "            [-1, 1, 'C2f', [16, True]],\n",
        "            [-1, 1, 'Conv', [32, 3, 2]],\n",
        "            [-1, 1, 'C2f', [32, True]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [-1, 1, 'C2f', [64, True]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [-1, 1, 'C2f', [128, True]],\n",
        "            [-1, 1, 'SPPF', [128, 5]],\n",
        "        ],\n",
        "        'head': [\n",
        "            [-1, 1, 'Conv', [64, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 6], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [64, False]],\n",
        "            [-1, 1, 'Conv', [32, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 4], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [32, False]],\n",
        "            [-1, 1, 'Conv', [32, 3, 2]],\n",
        "            [[-1, 14], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [64, False]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [[-1, 10], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [128, False]],\n",
        "            [[17, 20, 23], 1, 'Detect', ['nc']],\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    config_path = 'custom_yolo_ultra_minimal.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(custom_config, f)\n",
        "    return config_path\n",
        "\n",
        "def create_nano_yolo_config():\n",
        "    \"\"\"Create a nano YOLOv8 configurationâ€”very lightweight.\"\"\"\n",
        "    custom_config = {\n",
        "        'nc': 7,\n",
        "        'depth_multiple': 0.33,\n",
        "        'width_multiple': 0.25,\n",
        "        'backbone': [\n",
        "            [-1, 1, 'Conv', [16, 3, 2]],\n",
        "            [-1, 1, 'Conv', [32, 3, 2]],\n",
        "            [-1, 1, 'C2f', [32, True]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [-1, 2, 'C2f', [64, True]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [-1, 2, 'C2f', [128, True]],\n",
        "            [-1, 1, 'Conv', [256, 3, 2]],\n",
        "            [-1, 1, 'C2f', [256, True]],\n",
        "            [-1, 1, 'SPPF', [256, 5]],\n",
        "        ],\n",
        "        'head': [\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 6], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [128, False]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 4], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [64, False]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [[-1, 12], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [128, False]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [[-1, 9], 1, 'Concat', [1]],\n",
        "            [-1, 1, 'C2f', [256, False]],\n",
        "            [[15, 18, 21], 1, 'Detect', ['nc']],\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    config_path = 'custom_yolo_nano.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(custom_config, f)\n",
        "    return config_path\n",
        "\n",
        "def create_optimized_config():\n",
        "    \"\"\"Create optimized minimal YOLOv8 config for high precision.\"\"\"\n",
        "    config = {\n",
        "        'nc': 7,\n",
        "        'depth_multiple': 0.5,\n",
        "        'width_multiple': 0.5,\n",
        "        'backbone': [\n",
        "            [-1, 1, 'Conv', [32, 3, 2]],\n",
        "            [-1, 1, 'Conv', [64, 3, 2]],\n",
        "            [-1, 2, 'C2f', [64, True]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [-1, 3, 'C2f', [128, True]],\n",
        "            [-1, 1, 'Conv', [256, 3, 2]],\n",
        "            [-1, 3, 'C2f', [256, True]],\n",
        "            [-1, 1, 'Conv', [512, 3, 2]],\n",
        "            [-1, 2, 'C2f', [512, True]],\n",
        "            [-1, 1, 'SPPF', [512, 5]],\n",
        "        ],\n",
        "        'head': [\n",
        "            [-1, 1, 'Conv', [256, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 6], 1, 'Concat', [1]],\n",
        "            [-1, 2, 'C2f', [256, False]],\n",
        "            [-1, 1, 'Conv', [128, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 4], 1, 'Concat', [1]],\n",
        "            [-1, 2, 'C2f', [128, False]],\n",
        "            [-1, 1, 'Conv', [128, 3, 2]],\n",
        "            [[-1, 13], 1, 'Concat', [1]],\n",
        "            [-1, 2, 'C2f', [256, False]],\n",
        "            [-1, 1, 'Conv', [256, 3, 2]],\n",
        "            [[-1, 9], 1, 'Concat', [1]],\n",
        "            [-1, 2, 'C2f', [512, False]],\n",
        "            [[17, 20, 23], 1, 'Detect', ['nc']],\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    config_path = 'optimized_yolo.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "    return config_path\n",
        "\n",
        "def create_deep_multiscale_config():\n",
        "    \"\"\"Create deeper YOLOv8 config for multi-scale disease detection.\"\"\"\n",
        "    config = {\n",
        "        'nc': 7,\n",
        "        'depth_multiple': 0.67,\n",
        "        'width_multiple': 0.75,\n",
        "        'backbone': [\n",
        "            [-1, 1, 'Conv', [48, 3, 2]],\n",
        "            [-1, 2, 'Conv', [48, 3, 1]],\n",
        "            [-1, 1, 'Conv', [96, 3, 2]],\n",
        "            [-1, 3, 'C2f', [96, True]],\n",
        "            [-1, 1, 'Conv', [192, 3, 2]],\n",
        "            [-1, 4, 'C2f', [192, True]],\n",
        "            [-1, 1, 'Conv', [192, 3, 1]],\n",
        "            [-1, 1, 'Conv', [384, 3, 2]],\n",
        "            [-1, 6, 'C2f', [384, True]],\n",
        "            [-1, 1, 'Conv', [384, 3, 1]],\n",
        "            [-1, 1, 'Conv', [576, 3, 2]],\n",
        "            [-1, 3, 'C2f', [576, True]],\n",
        "            [-1, 1, 'SPPF', [576, 5]],\n",
        "            [-1, 1, 'Conv', [576, 1, 1]],\n",
        "        ],\n",
        "        'head': [\n",
        "            [-1, 1, 'Conv', [384, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 9], 1, 'Concat', [1]],\n",
        "            [-1, 3, 'C2f', [384, False]],\n",
        "            [-1, 1, 'Conv', [192, 1, 1]],\n",
        "            [-1, 1, 'nn.Upsample', [None, 2, 'nearest']],\n",
        "            [[-1, 6], 1, 'Concat', [1]],\n",
        "            [-1, 3, 'C2f', [192, False]],\n",
        "            [-1, 1, 'Conv', [192, 3, 2]],\n",
        "            [[-1, 18], 1, 'Concat', [1]],\n",
        "            [-1, 3, 'C2f', [384, False]],\n",
        "            [-1, 1, 'Conv', [384, 3, 2]],\n",
        "            [[-1, 14], 1, 'Concat', [1]],\n",
        "            [-1, 3, 'C2f', [576, False]],\n",
        "            [[21, 24, 27], 1, 'Detect', ['nc']],\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    config_path = 'deep_multiscale_yolo.yaml'\n",
        "    with open(config_path, 'w') as f:\n",
        "        yaml.dump(config, f)\n",
        "    return config_path\n",
        "\n",
        "# ============ High-Precision Training ============\n",
        "def train_high_precision(data_path, epochs=400):\n",
        "    \"\"\"Train for high mAP@50 and box precision.\"\"\"\n",
        "    config_path = create_optimized_config()\n",
        "    model = YOLO(config_path)\n",
        "\n",
        "    results = model.train(\n",
        "        data=data_path,\n",
        "        epochs=epochs,\n",
        "        imgsz=640,\n",
        "        batch=12,\n",
        "        lr0=0.005,\n",
        "        lrf=0.0001,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.001,\n",
        "        warmup_epochs=15,\n",
        "        warmup_momentum=0.5,\n",
        "        warmup_bias_lr=0.05,\n",
        "        box=10.0,\n",
        "        cls=1.0,\n",
        "        dfl=2.0,\n",
        "        hsv_h=0.01,\n",
        "        hsv_s=0.5,\n",
        "        hsv_v=0.3,\n",
        "        degrees=5,\n",
        "        translate=0.1,\n",
        "        scale=0.5,\n",
        "        shear=1.0,\n",
        "        perspective=0.0,\n",
        "        fliplr=0.5,\n",
        "        flipud=0.0,\n",
        "        mosaic=0.8,\n",
        "        mixup=0.05,\n",
        "        copy_paste=0.1,\n",
        "        optimizer='AdamW',\n",
        "        patience=100,\n",
        "        close_mosaic=50,\n",
        "        nbs=64,\n",
        "        save_period=10,\n",
        "        conf=0.001,\n",
        "        iou=0.6,\n",
        "        max_det=300,\n",
        "        device=0,\n",
        "        amp=True,\n",
        "        project='runs/precision',\n",
        "        name='high_precision_yolo',\n",
        "        exist_ok=True,\n",
        "        pretrained=False,\n",
        "        plots=True,\n",
        "        save=True,\n",
        "        save_txt=False,\n",
        "        save_conf=True,\n",
        "        save_crop=False,\n",
        "    )\n",
        "    return model, results\n",
        "\n",
        "# ============ Deep Multi-Scale Model Training ============\n",
        "def train_deep_multiscale(data_path, epochs=200):\n",
        "    \"\"\"Train deep multi-scale model for small/medium/large disease detection.\"\"\"\n",
        "    config_path = create_deep_multiscale_config()\n",
        "    model = YOLO(config_path)\n",
        "\n",
        "    precision_callback = PrecisionMAPCallback(precision_threshold=0.75)\n",
        "    model.add_callback(\"on_fit_epoch_end\", precision_callback)\n",
        "\n",
        "    results = model.train(\n",
        "        data=data_path,\n",
        "        epochs=epochs,\n",
        "        batch=8,\n",
        "        lr0=0.002,\n",
        "        lrf=0.00001,\n",
        "        momentum=0.937,\n",
        "        weight_decay=0.002,\n",
        "        warmup_epochs=20,\n",
        "        warmup_momentum=0.5,\n",
        "        warmup_bias_lr=0.01,\n",
        "        box=12.0,\n",
        "        cls=1.5,\n",
        "        dfl=2.5,\n",
        "        hsv_h=0.015,\n",
        "        hsv_s=0.6,\n",
        "        hsv_v=0.4,\n",
        "        degrees=10,\n",
        "        translate=0.15,\n",
        "        scale=0.7,\n",
        "        shear=2.0,\n",
        "        perspective=0.0001,\n",
        "        fliplr=0.5,\n",
        "        flipud=0.1,\n",
        "        mosaic=0.9,\n",
        "        mixup=0.15,\n",
        "        copy_paste=0.3,\n",
        "        optimizer='AdamW',\n",
        "        patience=150,\n",
        "        close_mosaic=100,\n",
        "        dropout=0.1,\n",
        "        imgsz=640,\n",
        "        rect=False,\n",
        "        save_period=20,\n",
        "        conf=0.001,\n",
        "        iou=0.5,\n",
        "        max_det=300,\n",
        "        device=0,\n",
        "        amp=True,\n",
        "        workers=8,\n",
        "        project='runs/deep_multiscale',\n",
        "        name='deep_disease_detector',\n",
        "        exist_ok=True,\n",
        "        pretrained=False,\n",
        "        plots=True,\n",
        "        save=True,\n",
        "        cache=True,\n",
        "    )\n",
        "    return model, results\n",
        "\n",
        "# ============ Small Dataset Optimization ============\n",
        "def train_for_small_dataset(data_path, epochs=600):\n",
        "    \"\"\"Special training strategy for small datasets with heavy regularization.\"\"\"\n",
        "    config_path = create_deep_multiscale_config()\n",
        "    model = YOLO(config_path)\n",
        "\n",
        "    # Stage 1: Pre-training with heavy augmentation\n",
        "    print(\"Stage 1: Pre-training with heavy augmentation...\")\n",
        "    model.train(\n",
        "        data=data_path,\n",
        "        epochs=200,\n",
        "        imgsz=512,\n",
        "        batch=4,\n",
        "        lr0=0.001,\n",
        "        mosaic=1.0,\n",
        "        mixup=0.3,\n",
        "        copy_paste=0.5,\n",
        "        degrees=20,\n",
        "        translate=0.3,\n",
        "        scale=0.9,\n",
        "        hsv_h=0.03,\n",
        "        hsv_s=0.8,\n",
        "        hsv_v=0.5,\n",
        "        weight_decay=0.005,\n",
        "        dropout=0.2,\n",
        "        save=False,\n",
        "        device=0,\n",
        "    )\n",
        "\n",
        "    # Stage 2: Fine-tuning with less augmentation\n",
        "    print(\"Stage 2: Fine-tuning with balanced augmentation...\")\n",
        "    model.train(\n",
        "        data=data_path,\n",
        "        epochs=400,\n",
        "        imgsz=640,\n",
        "        batch=6,\n",
        "        lr0=0.0005,\n",
        "        resume=True,\n",
        "        mosaic=0.7,\n",
        "        mixup=0.1,\n",
        "        copy_paste=0.2,\n",
        "        degrees=10,\n",
        "        translate=0.15,\n",
        "        scale=0.7,\n",
        "        weight_decay=0.002,\n",
        "        dropout=0.1,\n",
        "        box=15.0,\n",
        "        cls=2.0,\n",
        "        dfl=3.0,\n",
        "        project='runs/small_dataset',\n",
        "        name='final_model',\n",
        "        patience=200,\n",
        "        device=0,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ============ Multi-Scale Training for Better Generalization ============\n",
        "def train_multiscale(data_path, epochs=300):\n",
        "    \"\"\"Train with multiple scales for better mAP.\"\"\"\n",
        "    config_path = create_optimized_config()\n",
        "    model = YOLO(config_path)\n",
        "\n",
        "    # Stage 1: Small images for stability\n",
        "    print(\"Stage 1: Training on 416x416...\")\n",
        "    model.train(\n",
        "        data=data_path,\n",
        "        epochs=100,\n",
        "        imgsz=416,\n",
        "        batch=20,\n",
        "        lr0=0.01,\n",
        "        box=8.0,\n",
        "        cls=1.0,\n",
        "        save=False,\n",
        "        device=0,\n",
        "    )\n",
        "\n",
        "    # Stage 2: Medium images\n",
        "    print(\"Stage 2: Training on 512x512...\")\n",
        "    model.train(\n",
        "        data=data_path,\n",
        "        epochs=100,\n",
        "        imgsz=512,\n",
        "        batch=16,\n",
        "        lr0=0.005,\n",
        "        resume=True,\n",
        "        save=False,\n",
        "        device=0,\n",
        "    )\n",
        "\n",
        "    # Stage 3: Full resolution with fine-tuning\n",
        "    print(\"Stage 3: Fine-tuning on 640x640...\")\n",
        "    model.train(\n",
        "        data=data_path,\n",
        "        epochs=100,\n",
        "        imgsz=640,\n",
        "        batch=12,\n",
        "        lr0=0.001,\n",
        "        box=12.0,\n",
        "        resume=True,\n",
        "        project='runs/multiscale',\n",
        "        name='multiscale_yolo',\n",
        "        device=0,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ============ Ensemble Strategy ============\n",
        "def train_ensemble(data_path):\n",
        "    \"\"\"Train 3 models with different strategies for ensemble.\"\"\"\n",
        "    models = []\n",
        "\n",
        "    # Model 1: Standard training\n",
        "    print(\"Training Model 1: Standard\")\n",
        "    m1, _ = train_high_precision(data_path, epochs=300)\n",
        "    models.append(m1)\n",
        "\n",
        "    # Model 2: Heavy augmentation\n",
        "    print(\"Training Model 2: Heavy Augmentation\")\n",
        "    config_path = create_optimized_config()\n",
        "    m2 = YOLO(config_path)\n",
        "    m2.train(\n",
        "        data=data_path,\n",
        "        epochs=300,\n",
        "        imgsz=640,\n",
        "        batch=12,\n",
        "        mosaic=1.0,\n",
        "        mixup=0.2,\n",
        "        copy_paste=0.4,\n",
        "        degrees=15,\n",
        "        scale=0.9,\n",
        "        project='runs/ensemble',\n",
        "        name='model2_aug',\n",
        "    )\n",
        "    models.append(m2)\n",
        "\n",
        "    # Model 3: Different optimizer\n",
        "    print(\"Training Model 3: SGD Optimizer\")\n",
        "    config_path = create_optimized_config()\n",
        "    m3 = YOLO(config_path)\n",
        "    m3.train(\n",
        "        data=data_path,\n",
        "        epochs=300,\n",
        "        imgsz=640,\n",
        "        batch=12,\n",
        "        optimizer='SGD',\n",
        "        lr0=0.01,\n",
        "        momentum=0.937,\n",
        "        project='runs/ensemble',\n",
        "        name='model3_sgd',\n",
        "    )\n",
        "    models.append(m3)\n",
        "\n",
        "    return models\n",
        "\n",
        "# ============ Advanced Inference ============\n",
        "def precision_inference(image_path, model_path, conf_threshold=0.35):\n",
        "    \"\"\"Inference optimized for precision.\"\"\"\n",
        "    model = YOLO(model_path)\n",
        "    results_list = []\n",
        "\n",
        "    # Original image\n",
        "    results_list.append(model(image_path, conf=conf_threshold, iou=0.5))\n",
        "\n",
        "    # Flipped image\n",
        "    results_list.append(model(image_path, conf=conf_threshold, iou=0.5, fliplr=True))\n",
        "\n",
        "    # Different scales\n",
        "    for scale in [0.9, 1.0, 1.1]:\n",
        "        results_list.append(\n",
        "            model(image_path, conf=conf_threshold, iou=0.5, imgsz=int(640 * scale))\n",
        "        )\n",
        "\n",
        "    # Return results from the original image (as a simple example)\n",
        "    return results_list[0]\n",
        "\n",
        "# ============ Evaluation with Multiple Metrics ============\n",
        "def evaluate_precision(model_path, data_path):\n",
        "    \"\"\"Comprehensive evaluation for precision metrics.\"\"\"\n",
        "    model = YOLO(model_path)\n",
        "    best_conf = 0.25\n",
        "    best_map50 = 0\n",
        "\n",
        "    for conf in [0.15, 0.25, 0.35, 0.45, 0.55]:\n",
        "        metrics = model.val(data=data_path, conf=conf, iou=0.5)\n",
        "        map50 = metrics.box.map50\n",
        "        print(f\"Conf={conf}: mAP@50={map50:.4f}\")\n",
        "        if map50 > best_map50:\n",
        "            best_map50 = map50\n",
        "            best_conf = conf\n",
        "\n",
        "    print(f\"\\nBest mAP@50={best_map50:.4f} at conf={best_conf}\")\n",
        "\n",
        "    final_metrics = model.val(\n",
        "        data=data_path,\n",
        "        conf=best_conf,\n",
        "        iou=0.5,\n",
        "        save_json=True,\n",
        "        plots=True,\n",
        "    )\n",
        "    return final_metrics, best_conf\n",
        "\n",
        "# ============ Main Training Pipeline ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATA_PATH = '/content/drive/MyDrive/ELEC5304/dataset.yaml'\n",
        "\n",
        "    print(\"Training Deep Multi-Scale YOLOv8 for Disease Detection\")\n",
        "    print(\"Optimized for small, medium, and large disease detection\")\n",
        "    print(\"Will save model when precision â‰¥ 0.75 and continue for best mAP@50\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Train Deep Multi-Scale Model\n",
        "    model, results = train_deep_multiscale(DATA_PATH, epochs=500)\n",
        "\n",
        "    # Evaluate and find best threshold\n",
        "    print(\"\\nEvaluating model performance...\")\n",
        "    metrics, best_conf = evaluate_precision(\n",
        "        'runs/deep_multiscale/deep_disease_detector/weights/best.pt',\n",
        "        DATA_PATH\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"mAP@50: {metrics.box.map50:.4f}\")\n",
        "    print(f\"mAP@50-95: {metrics.box.map:.4f}\")\n",
        "    print(f\"Precision: {metrics.box.p:.4f}\")\n",
        "    print(f\"Recall: {metrics.box.r:.4f}\")\n",
        "    print(f\"Best confidence threshold: {best_conf}\")\n",
        "\n",
        "    # Performance by object size\n",
        "    print(\"\\nPerformance by object size:\")\n",
        "    print(f\"Small objects: mAP@50={metrics.box.maps[0]:.4f}\")\n",
        "    print(f\"Medium objects: mAP@50={metrics.box.maps[1]:.4f}\")\n",
        "    print(f\"Large objects: mAP@50={metrics.box.maps[2]:.4f}\")\n",
        "\n",
        "    # Example inference instruction\n",
        "    print(\"\\nFor inference, use:\")\n",
        "    print(\n",
        "        f\"results = precision_inference('image.jpg', \"\n",
        "        f\"'runs/deep_multiscale/deep_disease_detector/weights/best.pt', \"\n",
        "        f\"conf_threshold={best_conf})\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "NZaC_POcIphg",
        "outputId": "14ab5762-8f1b-4230-8584-33b9115c989d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Deep Multi-Scale YOLOv8 for Disease Detection\n",
            "Optimized for small, medium, and large disease detection\n",
            "Will save model when precision â‰¥ 0.75 and continue for best mAP@50\n",
            "--------------------------------------------------\n",
            "Ultralytics 8.3.146 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=12.0, cache=True, cfg=None, classes=None, close_mosaic=100, cls=1.5, conf=0.001, copy_paste=0.3, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/ELEC5304/dataset.yaml, degrees=10, deterministic=True, device=0, dfl=2.5, dnn=False, dropout=0.1, dynamic=False, embed=None, epochs=500, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.1, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.6, hsv_v=0.4, imgsz=640, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.002, lrf=1e-05, mask_ratio=4, max_det=300, mixup=0.15, mode=train, model=deep_multiscale_yolo.yaml, momentum=0.937, mosaic=0.9, multi_scale=False, name=deep_disease_detector, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=150, perspective=0.0001, plots=True, pose=12.0, pretrained=False, profile=False, project=runs/deep_multiscale, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/deep_multiscale/deep_disease_detector, save_frames=False, save_json=False, save_period=20, save_txt=False, scale=0.7, seed=0, shear=2.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.15, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.01, warmup_epochs=20, warmup_momentum=0.5, weight_decay=0.002, workers=8, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 15.3MB/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1160  ultralytics.nn.modules.conv.Conv             [3, 40, 3, 2]                 \n",
            "  1                  -1  1     14480  ultralytics.nn.modules.conv.Conv             [40, 40, 3, 1]                \n",
            "  2                  -1  1     26064  ultralytics.nn.modules.conv.Conv             [40, 72, 3, 2]                \n",
            "  3                  -1  2     62784  ultralytics.nn.modules.block.C2f             [72, 72, 2, True]             \n",
            "  4                  -1  1     93600  ultralytics.nn.modules.conv.Conv             [72, 144, 3, 2]               \n",
            "  5                  -1  3    353952  ultralytics.nn.modules.block.C2f             [144, 144, 3, True]           \n",
            "  6                  -1  1    186912  ultralytics.nn.modules.conv.Conv             [144, 144, 3, 1]              \n",
            "  7                  -1  1    373824  ultralytics.nn.modules.conv.Conv             [144, 288, 3, 2]              \n",
            "  8                  -1  4   1828224  ultralytics.nn.modules.block.C2f             [288, 288, 4, True]           \n",
            "  9                  -1  1    747072  ultralytics.nn.modules.conv.Conv             [288, 288, 3, 1]              \n",
            " 10                  -1  1   1120608  ultralytics.nn.modules.conv.Conv             [288, 432, 3, 2]              \n",
            " 11                  -1  2   2242944  ultralytics.nn.modules.block.C2f             [432, 432, 2, True]           \n",
            " 12                  -1  1    467856  ultralytics.nn.modules.block.SPPF            [432, 432, 5]                 \n",
            " 13                  -1  1    187488  ultralytics.nn.modules.conv.Conv             [432, 432, 1, 1]              \n",
            " 14                  -1  1    124992  ultralytics.nn.modules.conv.Conv             [432, 288, 1, 1]              \n",
            " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 16             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  2   1080576  ultralytics.nn.modules.block.C2f             [576, 288, 2, False]          \n",
            " 18                  -1  1     41760  ultralytics.nn.modules.conv.Conv             [288, 144, 1, 1]              \n",
            " 19                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 20             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2    270720  ultralytics.nn.modules.block.C2f             [288, 144, 2, False]          \n",
            " 22                  -1  1    186912  ultralytics.nn.modules.conv.Conv             [144, 144, 3, 2]              \n",
            " 23            [-1, 18]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 24                  -1  2    997632  ultralytics.nn.modules.block.C2f             [288, 288, 2, False]          \n",
            " 25                  -1  1    747072  ultralytics.nn.modules.conv.Conv             [288, 288, 3, 2]              \n",
            " 26            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 27                  -1  2   2305152  ultralytics.nn.modules.block.C2f             [576, 432, 2, False]          \n",
            " 28        [21, 24, 27]  1   2305909  ultralytics.nn.modules.head.Detect           [7, [144, 288, 432]]          \n",
            "deep_multiscale_YOLO summary: 177 layers, 15,767,693 parameters, 15,767,677 gradients, 51.8 GFLOPs\n",
            "\n",
            "Freezing layer 'model.28.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 64.7MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.5Â±0.1 ms, read: 0.1Â±0.0 MB/s, size: 34.1 KB)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/ELEC5304/train/labels.cache... 580 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580/580 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.7GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 580/580 [00:03<00:00, 145.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.7Â±0.3 ms, read: 0.1Â±0.0 MB/s, size: 33.9 KB)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/ELEC5304/test/labels.cache... 31 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:03<00:00,  8.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/deep_multiscale/deep_disease_detector/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.002), 87 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/deep_multiscale/deep_disease_detector\u001b[0m\n",
            "Starting training for 500 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      1/500      3.09G      5.711      14.74      6.703         36        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:29<00:00,  2.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         31        119    0.00106      0.164      0.012    0.00331\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      2/500       3.9G      5.836      12.85      5.885         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:21<00:00,  3.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.48it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         31        119       0.29      0.143     0.0403     0.0135\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      3/500         4G      5.291      11.13      5.258         23        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:22<00:00,  3.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.32it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         31        119      0.366     0.0972     0.0641     0.0212\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      4/500      4.05G      4.612      9.636      4.529         35        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:23<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.42it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         31        119     0.0807       0.43      0.161     0.0591\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      5/500      4.12G      4.326      8.807      4.093         23        640:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 71/73 [00:21<00:00,  3.85it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Trained Models"
      ],
      "metadata": {
        "id": "0SFuyW4F9fQ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgOSpyB5CnOV",
        "outputId": "f5cfed0c-d250-4175-e74f-a02f632e9643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using split from: /content/drive/MyDrive/Datasets/test/images\n",
            "Ultralytics 8.3.145 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (NVIDIA L4, 22693MiB)\n",
            "deep_multiscale_YOLO summary (fused): 96 layers, 15,753,981 parameters, 0 gradients, 51.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.3Â±0.1 ms, read: 37.5Â±3.9 MB/s, size: 66.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Datasets/test/labels.cache... 31 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         31        119      0.858      0.707      0.812      0.522\n",
            "Speed: 2.6ms preprocess, 8.8ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val22\u001b[0m\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0233_JPG.rf.b27c3e2dd843cab5e0f652fd1ce5659a.jpg: 640x640 4 early_blights, 10 late_blights, 36 spider_mitess, 4 target_spots, 19 mosaic_viruss, 10.8ms\n",
            "Speed: 2.4ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0249_JPG.rf.412df0b52b549fc121a346eb8a957ab0.jpg: 640x640 1 early_blight, 10 late_blights, 1 leaf_mold, 5 septoria_leaf_spots, 2 spider_mitess, 2 target_spots, 5 mosaic_viruss, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0277_JPG.rf.84073f240decbc79e30716373f3dd507.jpg: 640x640 8 late_blights, 3 leaf_molds, 6 spider_mitess, 1 mosaic_virus, 11.0ms\n",
            "Speed: 2.0ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0281_JPG.rf.a29c913abceeed1ea6092ab0f2c92b0d.jpg: 640x640 1 early_blight, 9 late_blights, 4 leaf_molds, 10 spider_mitess, 4 mosaic_viruss, 11.0ms\n",
            "Speed: 2.1ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0288_JPG.rf.ca824ec09b29571c39dc6b6ee6b0f6a7.jpg: 640x640 18 late_blights, 4 spider_mitess, 1 target_spot, 11 mosaic_viruss, 11.2ms\n",
            "Speed: 2.1ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0299_JPG.rf.d19f0cf7d3997c407f195ae8c59cd046.jpg: 640x640 8 late_blights, 6 septoria_leaf_spots, 1 spider_mites, 3 target_spots, 7 mosaic_viruss, 11.0ms\n",
            "Speed: 2.1ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0306_JPG.rf.8cfc41c387ad5f06390912b8adf4f001.jpg: 640x640 4 late_blights, 2 leaf_molds, 4 septoria_leaf_spots, 4 mosaic_viruss, 11.0ms\n",
            "Speed: 2.1ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0345_JPG.rf.6d1707b20ec861a78ba83945801a0898.jpg: 640x640 1 early_blight, 10 late_blights, 2 leaf_molds, 20 septoria_leaf_spots, 9 spider_mitess, 5 mosaic_viruss, 11.0ms\n",
            "Speed: 2.1ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0349_JPG.rf.986b62f1b9daaaea2228dff4cf072182.jpg: 640x640 2 early_blights, 14 late_blights, 8 septoria_leaf_spots, 1 spider_mites, 7 target_spots, 7 mosaic_viruss, 10.8ms\n",
            "Speed: 2.1ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0359_JPG.rf.e4fb9464647fd62372ad23e4e93acf58.jpg: 640x640 1 early_blight, 8 late_blights, 2 septoria_leaf_spots, 5 spider_mitess, 6 mosaic_viruss, 10.9ms\n",
            "Speed: 2.1ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0371_JPG.rf.eceaf990bab5276fbb1499eb0321f412.jpg: 640x640 2 early_blights, 13 late_blights, 1 leaf_mold, 4 spider_mitess, 6 mosaic_viruss, 11.8ms\n",
            "Speed: 2.1ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0554_JPG.rf.123e05abb041babb491a02b436260cff.jpg: 640x640 2 early_blights, 18 late_blights, 3 spider_mitess, 1 target_spot, 5 mosaic_viruss, 11.5ms\n",
            "Speed: 2.1ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0577_JPG.rf.8f4955468e2ee47f88e50443943f45df.jpg: 640x640 1 early_blight, 25 late_blights, 5 septoria_leaf_spots, 6 mosaic_viruss, 10.8ms\n",
            "Speed: 2.0ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0580_JPG.rf.1366720960579853ffc3981a6deaa5ab.jpg: 640x640 1 early_blight, 29 late_blights, 4 mosaic_viruss, 10.7ms\n",
            "Speed: 2.1ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0581_JPG.rf.303c1f0a2504708e64a37c20cf3e0d58.jpg: 640x640 27 late_blights, 1 septoria_leaf_spot, 1 target_spot, 4 mosaic_viruss, 10.9ms\n",
            "Speed: 2.0ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_0882_JPG.rf.63b0ec53b347103941c60dd138b64a64.jpg: 640x640 1 early_blight, 106 late_blights, 1 spider_mites, 2 target_spots, 12 mosaic_viruss, 12.3ms\n",
            "Speed: 2.1ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_1081_JPG.rf.af69c94dbae34f5bbbd988022910241a.jpg: 640x640 1 early_blight, 11 late_blights, 2 leaf_molds, 7 septoria_leaf_spots, 11 spider_mitess, 1 target_spot, 1 mosaic_virus, 11.2ms\n",
            "Speed: 2.1ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/Datasets/test/images/IMG_1128_JPG.rf.2b6c8bd27f4affcd740259c710285fa4.jpg: 640x640 5 early_blights, 57 late_blights, 1 leaf_mold, 5 septoria_leaf_spots, 1 spider_mites, 2 target_spots, 20 mosaic_viruss, 11.2ms\n",
            "Speed: 2.2ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "================ Evaluation Metrics ================\n",
            "\n",
            "Metric                              |     Value\n",
            "-------------------------------------------------\n",
            "Box precision (global)              |    0.8584\n",
            "Classâ€‘weighted mAP@50               |    0.8164\n",
            "AP_S  (small objects, IoU = 0.50)   |    0.7664\n",
            "-------------------------------------------------\n",
            "\n",
            "Perâ€‘class AP@50 and instance counts\n",
            "------------------------------------\n",
            "Class  0 | AP50:  0.750 | instances in test: 2\n",
            "Class  1 | AP50:  0.842 | instances in test: 66\n",
            "Class  2 | AP50:  0.995 | instances in test: 7\n",
            "Class  3 | AP50:  0.820 | instances in test: 20\n",
            "Class  4 | AP50:  0.648 | instances in test: 6\n",
            "Class  5 | AP50:  0.995 | instances in test: 4\n",
            "Class  6 | AP50:  0.633 | instances in test: 14\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Paths (update if you saved the model elsewhere)\n",
        "MODEL_PATH = 'runs/deep_multiscale/deep_disease_detector/weights/best_fourier.pt'\n",
        "DATA_YAML  = '/content/drive/MyDrive/Datasets/leaf7.yaml'  # dataset config\n",
        "\n",
        "# 1. Load the trained detector\n",
        "model = YOLO(MODEL_PATH)\n",
        "\n",
        "# 2. Build classâ€‘frequency weights from the TEST split\n",
        "with open(DATA_YAML) as f:\n",
        "    data_cfg = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "# -------------------- Resolve test/val split -------------------- #\n",
        "# Prefer an explicit 'test' split; otherwise fall back to 'val'.\n",
        "raw_test_root = data_cfg.get('test') or data_cfg.get('val')\n",
        "if raw_test_root is None:\n",
        "    raise ValueError(\n",
        "        \"Neither 'test' nor 'val' keys found in the dataset YAML. \"\n",
        "        \"Add a path, e.g.  test: /content/drive/.../images\"\n",
        "    )\n",
        "\n",
        "test_root = Path(raw_test_root)\n",
        "label_root = test_root.parent / 'labels'  # expects standard YOLO layout\n",
        "print(f\"Using split from: {test_root}\")\n",
        "\n",
        "class_counts = np.zeros(data_cfg['nc'], dtype=np.int64)\n",
        "for lb_file in label_root.glob('*.txt'):\n",
        "    for line in lb_file.read_text().splitlines():\n",
        "        cls_id = int(line.split()[0])\n",
        "        class_counts[cls_id] += 1\n",
        "\n",
        "if class_counts.sum() == 0:\n",
        "    raise RuntimeError(\"No labels found in the test set â€“ check DATA_YAML path\")\n",
        "\n",
        "class_weights = class_counts / class_counts.sum()\n",
        "\n",
        "# 3. Validate on the resolved split\n",
        "split_name = 'test' if data_cfg.get('test') else 'val'\n",
        "metrics = model.val(\n",
        "    data=DATA_YAML,\n",
        "    split=split_name,\n",
        "    conf=0.25,       # same threshold used during training evaluation\n",
        "    iou=0.50,        # mAP@50\n",
        "    verbose=False)\n",
        "\n",
        "\n",
        "if hasattr(metrics.box, \"tp_class\"):          # Older API\n",
        "    tp = np.array(metrics.box.tp_class)\n",
        "    fp = np.array(metrics.box.fp_class)\n",
        "    fn = np.array(metrics.box.fn_class)\n",
        "\n",
        "    total_tp = tp.sum()\n",
        "    total_fp = fp.sum()\n",
        "    total_fn = fn.sum()\n",
        "\n",
        "    box_precision = float(total_tp / (total_tp + total_fp + 1e-12))\n",
        "    box_recall    = float(total_tp / (total_tp + total_fn + 1e-12))\n",
        "else:\n",
        "    mp_obj = metrics.box.mp\n",
        "    mr_obj = metrics.box.mr\n",
        "\n",
        "    # Handle both callable and attribute variants\n",
        "    box_precision = float(mp_obj() if callable(mp_obj) else mp_obj)\n",
        "    box_recall    = float(mr_obj() if callable(mr_obj) else mr_obj)\n",
        "\n",
        "# Per-class AP@50 (exact IoU = 0.50)\n",
        "if hasattr(metrics.box, \"all_ap\") and isinstance(metrics.box.all_ap, (list, np.ndarray)):\n",
        "    per_class_ap50 = np.array(metrics.box.all_ap)[:, 0]          # first column is IoU 0.50\n",
        "elif hasattr(metrics.box, \"ap50\") and not callable(metrics.box.ap50):\n",
        "    per_class_ap50 = np.array(metrics.box.ap50)                  # already perâ€‘class array\n",
        "elif hasattr(metrics.box, \"ap50\") and callable(metrics.box.ap50):\n",
        "    per_class_ap50 = np.array(metrics.box.ap50())\n",
        "elif hasattr(metrics.box, \"ap50_class\"):\n",
        "    per_class_ap50 = np.array(metrics.box.ap50_class)\n",
        "else:\n",
        "    per_class_ap50 = np.array(metrics.box.maps)                  # fallback (IoU sweep)\n",
        "\n",
        "# Classâ€‘weighted mAP@50\n",
        "if per_class_ap50.shape[0] != class_weights.shape[0]:\n",
        "    raise ValueError(\n",
        "        f\"Mismatch between perâ€‘class AP array ({per_class_ap50.shape[0]}) \"\n",
        "        f\"and classâ€‘weight array ({class_weights.shape[0]}). \"\n",
        "        \"Check the metric extraction logic.\"\n",
        "    )\n",
        "weighted_map50 = float(np.dot(per_class_ap50, class_weights))\n",
        "\n",
        "# Custom AP_S @ 0.50 computed directly from predictions#\n",
        "# This routine scans the test images, collects all groundâ€‘truth and predicted\n",
        "# boxes whose area is < 32^2 pixels (COCO definition of \"small\"), and then\n",
        "# computes AP at IoU 0.50 from scratch.\n",
        "\n",
        "import PIL.Image as Image\n",
        "\n",
        "def compute_ap_s50(model, test_root, label_root, iou_thr=0.50, area_thr=32**2):\n",
        "    \"\"\"Return AP for 'small' objects (area < area_thr) at a fixed IoU.\"\"\"\n",
        "    all_scores, all_tp = [], []\n",
        "    num_gt = 0\n",
        "\n",
        "    # Iterate through every image in the split\n",
        "    for img_path in sorted(test_root.glob('*')):\n",
        "        if img_path.suffix.lower() not in {'.jpg', '.jpeg', '.png'}:\n",
        "            continue\n",
        "\n",
        "        # Load GT boxes\n",
        "        lbl_file = label_root / f\"{img_path.stem}.txt\"\n",
        "        if not lbl_file.exists():\n",
        "            continue\n",
        "        gts = []\n",
        "        w, h = Image.open(img_path).size\n",
        "        for line in lbl_file.read_text().splitlines():\n",
        "            _, cx, cy, bw, bh = map(float, line.strip().split())\n",
        "            x1 = (cx - bw / 2) * w\n",
        "            y1 = (cy - bh / 2) * h\n",
        "            x2 = (cx + bw / 2) * w\n",
        "            y2 = (cy + bh / 2) * h\n",
        "            if (x2 - x1) * (y2 - y1) < area_thr:\n",
        "                gts.append([x1, y1, x2, y2])\n",
        "        num_gt += len(gts)\n",
        "        if len(gts) == 0:\n",
        "            continue\n",
        "        gts = np.array(gts)\n",
        "\n",
        "        # Run prediction with low conf to capture all\n",
        "        preds = model(img_path, conf=0.001, iou=iou_thr)[0]\n",
        "        if preds.boxes.shape[0] == 0:\n",
        "            continue\n",
        "        boxes = preds.boxes.xyxy.cpu().numpy()\n",
        "        scores = preds.boxes.conf.cpu().numpy()\n",
        "\n",
        "        # Filter predictions whose box area is small\n",
        "        areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "        mask_small = areas < area_thr\n",
        "        boxes = boxes[mask_small]\n",
        "        scores = scores[mask_small]\n",
        "        if boxes.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Sort by confidence descending\n",
        "        order = scores.argsort()[::-1]\n",
        "        boxes = boxes[order]\n",
        "        scores = scores[order]\n",
        "\n",
        "        # Match predictions to GT\n",
        "        matched = np.zeros(len(gts), dtype=bool)\n",
        "        for box, sc in zip(boxes, scores):\n",
        "            ious = (\n",
        "                np.maximum(0, np.minimum(box[2], gts[:, 2]) - np.maximum(box[0], gts[:, 0]))\n",
        "                * np.maximum(0, np.minimum(box[3], gts[:, 3]) - np.maximum(box[1], gts[:, 1]))\n",
        "            )\n",
        "            inter = ious\n",
        "            union = (\n",
        "                (box[2] - box[0]) * (box[3] - box[1])\n",
        "                + (gts[:, 2] - gts[:, 0]) * (gts[:, 3] - gts[:, 1])\n",
        "                - inter\n",
        "            )\n",
        "            ious = inter / (union + 1e-6)\n",
        "            best_idx = np.argmax(ious)\n",
        "            if ious[best_idx] >= iou_thr and not matched[best_idx]:\n",
        "                all_tp.append(1)\n",
        "                matched[best_idx] = True\n",
        "            else:\n",
        "                all_tp.append(0)\n",
        "            all_scores.append(sc)\n",
        "\n",
        "    if num_gt == 0:\n",
        "        return float('nan')\n",
        "\n",
        "    # Compute precisionâ€‘recall and AP\n",
        "    all_scores = np.array(all_scores)\n",
        "    all_tp = np.array(all_tp)\n",
        "    if all_scores.size == 0:\n",
        "        return 0.0\n",
        "    idx = all_scores.argsort()[::-1]\n",
        "    all_tp = all_tp[idx]\n",
        "\n",
        "    tp_cum = np.cumsum(all_tp)\n",
        "    fp_cum = np.cumsum(1 - all_tp)\n",
        "    recalls = tp_cum / (num_gt + 1e-12)\n",
        "    precisions = tp_cum / (tp_cum + fp_cum + 1e-12)\n",
        "\n",
        "    # 11â€‘point interpolation (VOC 2007 style, sufficient for single IoU)\n",
        "    ap = 0.0\n",
        "    for t in np.linspace(0, 1, 11):\n",
        "        if np.any(recalls >= t):\n",
        "            ap += precisions[recalls >= t].max()\n",
        "    ap /= 11\n",
        "    return ap\n",
        "\n",
        "ap_s = compute_ap_s50(model, test_root, label_root)\n",
        "\n",
        "\n",
        "# 5. Present the results\n",
        "print(\"\\n================ Evaluation Metrics ================\\n\")\n",
        "print(f\"{'Metric':35s} | {'Value':>9s}\")\n",
        "print(\"-\" * 49)\n",
        "print(f\"{'Box precision (global)':35s} | {box_precision:9.4f}\")\n",
        "print(f\"{'Classâ€‘weighted mAP@50':35s} | {weighted_map50:9.4f}\")\n",
        "print(f\"{'AP_S  (small objects, IoU = 0.50)':35s} | {ap_s:9.4f}\")\n",
        "print(\"-\" * 49 + \"\\n\")\n",
        "\n",
        "print(\"Perâ€‘class AP@50 and instance counts\")\n",
        "print(\"------------------------------------\")\n",
        "for cls_id, (ap, cnt) in enumerate(zip(per_class_ap50, class_counts)):\n",
        "    print(f\"Class {cls_id:2d} | AP50: {ap:6.3f} | instances in test: {cnt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2fpp6ffqOY4"
      },
      "source": [
        "MODEL 2\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}